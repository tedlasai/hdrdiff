{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d063a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8756025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RouteByType operator_map: [(<class 'str'>, <diffsynth.trainers.stuttgart_dataset.DataProcessingPipeline object at 0x14eea43e2a40>)]\n",
      "MAX: 604.0 min: -0.09844971\n",
      "Image.shape (1080, 1920, 3) max value: 16.0\n",
      "tensor shape: torch.Size([3, 1080, 1920]) max value: tensor(16.)\n",
      "MAX: 581.0 min: -0.10961914\n",
      "Image.shape (1080, 1920, 3) max value: 16.0\n",
      "tensor shape: torch.Size([3, 1080, 1920]) max value: tensor(16.)\n",
      "MAX: 630.0 min: -0.11773682\n",
      "Image.shape (1080, 1920, 3) max value: 16.0\n",
      "tensor shape: torch.Size([3, 1080, 1920]) max value: tensor(16.)\n",
      "MAX: 630.0 min: -0.070495605\n",
      "Image.shape (1080, 1920, 3) max value: 16.0\n",
      "tensor shape: torch.Size([3, 1080, 1920]) max value: tensor(16.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving video:   0%|          | 0/13 [00:00<?, ?it/s]Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Saving video:   8%|▊         | 1/13 [00:00<00:02,  4.41it/s]Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Saving video: 100%|██████████| 13/13 [00:00<00:00, 39.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from: ./models/Wan-AI/Wan2.2-TI2V-5B/models_t5_umt5-xxl-enc-bf16.pth\n",
      "    model_name: wan_video_text_encoder model_class: WanTextEncoder\n",
      "    The following models are loaded: ['wan_video_text_encoder'].\n",
      "Loading models from: ['./models/Wan-AI/Wan2.2-TI2V-5B/diffusion_pytorch_model-00003-of-00003-bf16.safetensors', './models/Wan-AI/Wan2.2-TI2V-5B/diffusion_pytorch_model-00002-of-00003-bf16.safetensors', './models/Wan-AI/Wan2.2-TI2V-5B/diffusion_pytorch_model-00001-of-00003-bf16.safetensors']\n",
      "    model_name: wan_video_dit model_class: WanModel\n",
      "        This model is initialized with extra kwargs: {'has_image_input': False, 'patch_size': [1, 2, 2], 'in_dim': 48, 'dim': 3072, 'ffn_dim': 14336, 'freq_dim': 256, 'text_dim': 4096, 'out_dim': 48, 'num_heads': 24, 'num_layers': 30, 'eps': 1e-06, 'seperated_timestep': True, 'require_clip_embedding': False, 'require_vae_embedding': False, 'fuse_vae_embedding_in_latents': True}\n",
      "    The following models are loaded: ['wan_video_dit'].\n",
      "Loading models from: ./models/Wan-AI/Wan2.2-TI2V-5B/Wan2.2_VAE.pth\n",
      "    model_name: wan_video_vae model_class: WanVideoVAE38\n",
      "    The following models are loaded: ['wan_video_vae'].\n",
      "Using wan_video_text_encoder from ./models/Wan-AI/Wan2.2-TI2V-5B/models_t5_umt5-xxl-enc-bf16.pth.\n",
      "Using wan_video_dit from ['./models/Wan-AI/Wan2.2-TI2V-5B/diffusion_pytorch_model-00003-of-00003-bf16.safetensors', './models/Wan-AI/Wan2.2-TI2V-5B/diffusion_pytorch_model-00002-of-00003-bf16.safetensors', './models/Wan-AI/Wan2.2-TI2V-5B/diffusion_pytorch_model-00001-of-00003-bf16.safetensors'].\n",
      "Using wan_video_vae from ./models/Wan-AI/Wan2.2-TI2V-5B/Wan2.2_VAE.pth.\n",
      "No wan_video_image_encoder models available.\n",
      "No wan_video_motion_controller models available.\n",
      "No wan_video_vace models available.\n",
      "No wans2v_audio_encoder models available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:42<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/data2/saikiran.tedla/hdrvideo/diff\")\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from diffsynth import save_video, VideoData, load_state_dict\n",
    "from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig\n",
    "from modelscope import dataset_snapshot_download\n",
    "\n",
    "from diffsynth.trainers.stuttgart_dataset import StuttgartDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = StuttgartDataset(\n",
    "    base_path=\"/data2/saikiran.tedla/hdrvideo/diff/data/stuttgart/carousel_fireworks_02\",\n",
    "    repeat=1,\n",
    "    main_data_operator=StuttgartDataset.default_video_operator(\n",
    "        base_path=\"/data2/saikiran.tedla/hdrvideo/diff/data/stuttgart/carousel_fireworks_02\",\n",
    "        max_pixels=1280*720,\n",
    "        height=480,\n",
    "        width=832,\n",
    "        height_division_factor=16,\n",
    "        width_division_factor=16,\n",
    "        num_frames=13,\n",
    "        time_division_factor=4,\n",
    "        time_division_remainder=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Get a sample from the dataset\n",
    "data = dataset[0]\n",
    "condition_video = data[\"video\"]\n",
    "save_video(condition_video, \"stuttgart_input.mp4\", fps=15)\n",
    "\n",
    "pipe = WanVideoPipeline.from_pretrained(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    "    model_configs=[\n",
    "        ModelConfig(model_id=\"Wan-AI/Wan2.2-TI2V-5B\", origin_file_pattern=\"models_t5_umt5-xxl-enc-bf16.pth\", offload_device=\"cpu\", skip_download=True),\n",
    "        ModelConfig(model_id=\"Wan-AI/Wan2.2-TI2V-5B\", origin_file_pattern=\"diffusion_pytorch_model*.safetensors\", offload_device=\"cpu\", skip_download=True),\n",
    "        ModelConfig(model_id=\"Wan-AI/Wan2.2-TI2V-5B\", origin_file_pattern=\"Wan2.2_VAE.pth\", offload_device=\"cpu\", skip_download=True),\n",
    "    ],\n",
    ")\n",
    "state_dict = load_state_dict(\"/data2/saikiran.tedla/hdrvideo/diff/models/train/firstweek1e4/checkpoints/epoch-25.safetensors\")\n",
    "pipe.dit.load_state_dict(state_dict)\n",
    "pipe.enable_vram_management()\n",
    "\n",
    "video = pipe(\n",
    "    prompt=\"\",\n",
    "    #negative_prompt=\"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\",\n",
    "    condition_video=condition_video[0:5],  # Use the first 5 frames as condition\n",
    "    num_frames=13,\n",
    "    seed=1, tiled=False,\n",
    "    cfg_scale=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf3352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving video:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Saving video: 100%|██████████| 13/13 [00:00<00:00, 22.77it/s]\n"
     ]
    }
   ],
   "source": [
    "pil_video = pipe.vae_output_to_video(video)\n",
    "save_video(pil_video, \"stuttgart_output.mp4\", fps=15, quality=5)\n",
    "# #write out video as npy array\n",
    "import numpy as np\n",
    "video_cpu = video.cpu().to(torch.float32).numpy()[0]\n",
    "\n",
    "np.save(\"stuttgart_output.npy\", video_cpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anyedit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
